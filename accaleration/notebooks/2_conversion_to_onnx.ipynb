{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab8168cd",
   "metadata": {},
   "source": [
    "# Conversion to ONNX\n",
    "\n",
    "In this notebook, the conversion of the model to ONNX format is presented. The whole process contains the following step:\n",
    "\n",
    "* Isolating the actor from the actor-critic model since it is the only one responsible for producing actions.\n",
    "* Save a collection of states for later validation of the ONNX model's outputs.\n",
    "* Converting the actor to ONNX format.\n",
    "* Validating the ONNX model using the collected states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57ea190b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:commons:version: 6.2.4 *\n",
      "DEBUG:typing:version: 6.2.3\n",
      "DEBUG:duckietown_world:duckietown-world version 6.2.38 path /home/kanagnostopoulos/anaconda3/envs/rl/lib/python3.8/site-packages\n",
      "DEBUG:geometry:PyGeometry-z6 version 2.1.4 path /home/kanagnostopoulos/anaconda3/envs/rl/lib/python3.8/site-packages\n",
      "DEBUG:aido_schemas:aido-protocols version 6.0.59 path /home/kanagnostopoulos/anaconda3/envs/rl/lib/python3.8/site-packages\n",
      "DEBUG:nodes:version 6.2.13 path /home/kanagnostopoulos/anaconda3/envs/rl/lib/python3.8/site-packages pyparsing 3.0.6\n",
      "DEBUG:gym-duckietown:gym-duckietown version 6.1.31 path /home/kanagnostopoulos/Desktop/ReinforcementLearning/gym-duckietown/src\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': ('xaudio2', 'directsound', 'openal', 'pulse', 'silent'), 'debug_font': False, 'debug_gl': True, 'debug_gl_trace': False, 'debug_gl_trace_args': False, 'debug_graphics_batch': False, 'debug_lib': False, 'debug_media': False, 'debug_texture': False, 'debug_trace': False, 'debug_trace_args': False, 'debug_trace_depth': 1, 'debug_trace_flush': True, 'debug_win32': False, 'debug_x11': False, 'graphics_vbo': True, 'shadow_window': True, 'vsync': None, 'xsync': True, 'xlib_fullscreen_override_redirect': False, 'darwin_cocoa': True, 'search_local_libs': True, 'headless': False, 'headless_device': 0}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../../learning/src')\n",
    "\n",
    "import torch, logging, joblib, onnxruntime, numpy as np\n",
    "from torchvision import transforms as T\n",
    "from torch.nn import functional as F \n",
    "from gym_duckietown.simulator import Simulator\n",
    "from big_experiment_utils.wrappers import DiscreteWrapper, DtRewardWrapper\n",
    "from collections import deque\n",
    "\n",
    "from utilities import environment_creator\n",
    "\n",
    "logger = logging.getLogger('gym-duckietown')\n",
    "logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949a8db7",
   "metadata": {},
   "source": [
    "## Isolation of the Actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebc8f3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:duckietown_world: data: /home/kanagnostopoulos/anaconda3/envs/rl/lib/python3.8/site-packages/duckietown_world/data\n"
     ]
    }
   ],
   "source": [
    "env = environment_creator()\n",
    "\n",
    "model = joblib.load('../models/duckieTown_PPO_simple.joblib')\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    #Implements the inference of only the actor model coming from the acotr critic object\n",
    "\n",
    "    def __init__(self, \n",
    "                actor_critic_model,\n",
    "                device = 'cpu'):\n",
    "        #Initilizes actor by copying necesseary layers\n",
    "        super(Actor, self).__init__()\n",
    "        self.device = device\n",
    "        self.conv_core = actor_critic_model.conv_core.to(device)\n",
    "        self.actor_head = actor_critic_model.actor_head.to(device)\n",
    "        self.transform = actor_critic_model.transform\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Implements forward pass of model \n",
    "        x = torch.permute(x, (0, 3, 1, 2))   # Place channel axis in correct position\n",
    "        x = self.transform(x)               # Apply transform\n",
    "        #x = x / 255\n",
    "        x = T.functional.crop(x, top=20, left=0, height=40, width=80)\n",
    "        #x = x[:,:,20:,:]\n",
    "        x = x.to(device=self.device)\n",
    "        visual_repr = self.conv_core(x).squeeze(-1).squeeze(-1)  \n",
    "        dist = F.log_softmax(self.actor_head(visual_repr), dim=1).to('cpu')\n",
    "        return dist\n",
    "    \n",
    "    def infer_action(self, x):\n",
    "        # Utilizes torch distributions to return an action\n",
    "        dist_probs = self.forward(x)\n",
    "        dist = torch.distributions.Categorical(logits=dist_probs)\n",
    "        return dist.sample().cpu().numpy()[0]\n",
    "    \n",
    "model = Actor(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d285a239",
   "metadata": {},
   "source": [
    "## Collect environment states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df5a8481",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_states = []\n",
    "\n",
    "frame = env.reset()\n",
    "stacked_frames = deque([torch.zeros(size=frame.shape).unsqueeze(0)]*5,\n",
    "                        maxlen=5)\n",
    "env.render()\n",
    "\n",
    "for _ in range(0,500):\n",
    "    \n",
    "    frame = torch.FloatTensor(frame).unsqueeze(0)\n",
    "    stacked_frames.append(frame)\n",
    "    state = torch.cat(tuple(stacked_frames), dim=-1)\n",
    "    \n",
    "    saved_states.append(state)\n",
    "    action = model.infer_action(state)\n",
    "    next_frame, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    frame = next_frame\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb14d0ec",
   "metadata": {},
   "source": [
    "## Convert Pytorch model to ONNX format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7c93dad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = saved_states[10] # Randomly selected input state\n",
    "\n",
    "torch.onnx.export(model,                              # model being run\n",
    "                  dummy_input,     # model input (or a tuple for multiple inputs)\n",
    "                  \"../models/actor.onnx\",    # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=11,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output'], # the model's output names\n",
    "                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "                                'output' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6539be9b",
   "metadata": {},
   "source": [
    "For easier use, an ONNXActor class is implemented that implements the same methods as the Actor class with the only difference being the fact the _forward()_ is based on _onnxruntime.InferenceSession()_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8b0f787c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ONNXActor():\n",
    "    # Implements actor using ONNX runtime\n",
    "    \n",
    "    def __init__(self, onnx_path, providers):\n",
    "        # Initiliaze model\n",
    "        self.ort_session = onnxruntime.InferenceSession(onnx_path, providers=providers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Implements forward pass of model\n",
    "        output = self.ort_session.run(None, {'input' : x.numpy().astype(np.float32)})[0]\n",
    "        return torch.Tensor(output)\n",
    "    \n",
    "    def infer_action(self, x):\n",
    "        # Utilizes torch distributions to return an action\n",
    "        dist_probs = self.forward(x)\n",
    "        dist = torch.distributions.Categorical(logits=dist_probs)\n",
    "        return dist.sample().numpy()[0]\n",
    "\n",
    "\n",
    "model_onnx = ONNXActor(onnx_path='../models/actor.onnx', providers=['CPUExecutionProvider'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41e095a",
   "metadata": {},
   "source": [
    "## Validating correctness of ONNX model\n",
    "\n",
    "Using the collected environment states, we validate that the initial Pytorch model produces the same probabilities as the ONNX based model with an accuracy up to 6th decimal digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f291a151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed\n"
     ]
    }
   ],
   "source": [
    "batch_input  = torch.cat(saved_states)\n",
    "\n",
    "golden_probs = model(batch_input).detach().numpy()\n",
    "onnx_probs = model_onnx.forward(batch_input).numpy()\n",
    "\n",
    "try:\n",
    "    np.testing.assert_array_almost_equal(golden_probs, onnx_probs, decimal=6)\n",
    "    print('Test passed')\n",
    "except:\n",
    "    print('Test failed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
